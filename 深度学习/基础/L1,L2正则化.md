<center><b>L1、L2正则化</b></center>

$L_{1}(w)=\Sigma_{i}\left|w_{i}\right|$
$L_{2}(w)=\frac{1}{2} \Sigma_{i} w_{i}^{2}$

L1正则化假设权重先验服从拉普拉斯分布，L2正则化假设权重先验服从高斯分布。  

为什么 L1 有稀疏权重的效果， L2 有让权重都变小的效果？  
[Why L1 norm for sparse models](https://stats.stackexchange.com/questions/45643/why-l1-norm-for-sparse-models/45644#45644?newreg=605f04b4f4e14fdf8d2b79a917922a85)
![l1l2](resource/L1,L2正则化/l1l2.png)


严格的推导见 《深度学习》 7.1.1、7.1.2  

L1正则化由于稀疏性，常被用于特征选择。  

https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a

https://www.quora.com/Why-is-L1-regularization-supposed-to-lead-to-sparsity-than-L2

https://medium.com/datadriveninvestor/l1-l2-regularization-7f1b4fe948f2

https://lumingdong.cn/detailed-summary-of-regularization.html#%E8%B4%9D%E5%8F%B6%E6%96%AF%E8%A7%92%E5%BA%A6%E7%90%86%E8%A7%A3%E6%AD%A3%E5%88%99%E5%8C%96